{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for text detection in images.\n",
    "Returns a list of image file paths with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, UnidentifiedImageError\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the OCR reader\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Store paths of images with text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m images_with_text \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\easyocr\\easyocr.py:214\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[0;32m    211\u001b[0m     dict_list[lang] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m, lang \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detector:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recognizer:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recog_network \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\easyocr\\easyocr.py:271\u001b[0m, in \u001b[0;36mReader.initDetector\u001b[1;34m(self, detector_path)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitDetector\u001b[39m(\u001b[38;5;28mself\u001b[39m, detector_path):\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcudnn_benchmark\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn_benchmark\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\easyocr\\detection.py:85\u001b[0m, in \u001b[0;36mget_detector\u001b[1;34m(trained_model, device, quantize, cudnn_benchmark)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     net\u001b[38;5;241m.\u001b[39mload_state_dict(copyStateDict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m))\n\u001b[0;32m     86\u001b[0m     net \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(net)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     87\u001b[0m     cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m cudnn_benchmark\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:1495\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[0;32m   1496\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[0;32m   1497\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:1754\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1752\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1753\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1754\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1756\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:1682\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1680\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Storage, torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes))\n\u001b[0;32m   1681\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m   1686\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mobj, dtype\u001b[38;5;241m=\u001b[39mdtype, _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:1801\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[1;32m-> 1801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:693\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\serialization.py:632\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m    631\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\storage.py:292\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m    291\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\tf_env39\\lib\\site-packages\\torch\\_utils.py:100\u001b[0m, in \u001b[0;36m_to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse\n\u001b[0;32m     98\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 100\u001b[0m \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import easyocr\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# Initialize the OCR reader\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Store paths of images with text\n",
    "images_with_text = []\n",
    "\n",
    "# Root directory\n",
    "root_dir = r\"C:\\Users\\sacar\\OneDrive\\Documents\\Semester 2 NOVA\\DL\\DeepLearning2425\\rare_species\"\n",
    "\n",
    "# Loop through folders and files\n",
    "for folder_name in os.listdir(root_dir):\n",
    "    folder_path = os.path.join(root_dir, folder_name)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue  # Skip non-folders\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Only process image files\n",
    "        if not file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Check for image validity first\n",
    "            with Image.open(file_path) as img:\n",
    "                img.verify()  # Will throw if file is invalid\n",
    "\n",
    "            # Actually run OCR\n",
    "            has_text = bool(reader.readtext(file_path))\n",
    "            if has_text:\n",
    "                images_with_text.append(file_path)\n",
    "                print(f\"Found text in: {file_path}\")\n",
    "\n",
    "        #Exception\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Done. Found {len(images_with_text)} images with text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_with_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m \u001b[43mimages_with_text\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(link)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images_with_text' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
